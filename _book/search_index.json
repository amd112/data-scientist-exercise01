[
["index.html", "Predicting Income From 1996 US Census Data", " Predicting Income From 1996 US Census Data Using an available US Census microdata sample from 1996, we attempt to predict if individuals have an income of above or below $50,000. Before broaching the ultimate modeling goal, it is important to understand the quirks of the data, and any data cleansing issues that must be addressed. Overall, the data doesn’t follow some patterns we expect. By comparing the 1996 microdata sample to the aggregations of surrounding decennial census’, we see this is likely not a random sample. There is significant underrepresentation of the black and ‘other’ race groups, as well as a clear overrepresentation of the male population (which composes 66.85% of the sample). Exploring other common trends in earnings, we see expected relationships. Among participants with more years of education, there is a higher percentage of respondents with incomes over 50k. Similarly, there is some relationship between hours worked and percentage of respondents with incomes over 50k. Using these (and other) relationships in the data, we model the probability of an individual record having an income of over 10k. "],
["modeling.html", "Modeling", " Modeling split in to test training and validation model to predict over or under 50k "],
["code.html", "Code", " Code #importing relevant libraries library(randomForest) library(e1071) library(ggplot2) library(dplyr) library(gridExtra) #setting seed to ensure that randomized calculations are constant set.seed(50) Code to flatten the database: library(DBI) library(RSQLite) con = dbConnect(SQLite(), dbname = &quot;exercise01.sqlite&quot;) dbExecute(con, &quot;CREATE TABLE flat (id INT PRIMARY KEY, age INT, workclass NCHAR(50), education NCHAR(50), education_id INT, marital_status NCHAR(50), occupation NCHAR(50), relationship NCHAR(50), race NCHAR(30), sex NCHAR(10), capital_gain INT, capital_loss INT, hours_week INT, country NCHAR(60), over_50k INT);&quot;) dbExecute(con, &quot;INSERT INTO flat SELECT records.id, records.age, w.name, e.name, records.education_num, m.name, o.name, re.name, r.name, s.name, capital_gain, capital_loss, hours_week, c.name, over_50k FROM countries AS c, education_levels AS e, marital_statuses AS m, occupations AS o, races AS r, records, relationships AS re, sexes AS s, workclasses AS w WHERE records.workclass_id = w.id AND records.education_level_id = e.id AND records.marital_status_id = m.id AND records.occupation_id = o.id AND records.relationship_id = re.id AND records.race_id = r.id AND records.sex_id = s.id AND records.country_id = c.id;&quot;) flat = dbGetQuery(con, &quot;SELECT * FROM flat;&quot;) write.csv(flat, file = &quot;flat.csv&quot;, row.names = FALSE) #importing file, removing column of ids flat = read.csv(&quot;flat.csv&quot;)[, -1] flat$over_50k = as.factor(flat$over_50k) #chunk is pushed right by the html wrapper #getting the summary of race distribution from the full data temp = as.data.frame(summary(flat$race)/nrow(flat)) names(temp) = c(&quot;percentages&quot;) temp$race = rownames(temp) rownames(temp) = c() temp$year = 1996 #appending the summaries of race distributions for 1990 and 2000 ninety = data.frame(percentages = c(0.008, 0.029, 0.121, 0.039, 0.803), race = temp$race, year = 1990) thou = data.frame(percentages = c(0.009, 0.037, 0.123, 0.08, 0.751), race = temp$race, year = 2000) temp = rbind(temp, ninety, thou) temp$year = as.factor(temp$year) #source 1990, page 3: https://www2.census.gov/library/publications/decennial/1990/cp-1/cp-1-1.pdf #source 2000, page 2: https://www.census.gov/prod/2002pubs/c2kprof00-us.pdf #plotting the summaries in comparison ggplot(temp) + geom_bar(aes(x = year, y = percentages, fill = race), position = &quot;stack&quot;, stat = &quot;identity&quot;) + ylim(0, 1) + xlab(&quot;Census Year&quot;) + ylab(&quot;Percent&quot;) #splitting the data in to train, validate, and testing groups s = 1:nrow(flat) train = sample(s, 12000) validate = sample(s[!s %in% train], 4000) test = s[(!s %in% train) &amp; (!s %in% validate)] n = 10 #creating and testing logisitic regression l = glm(over_50k ~ ., data = flat[train, ], family = binomial(link = &quot;logit&quot;)) p = predict(l, flat[validate, ], type = &quot;response&quot;) perc = (p&gt;0.05) == (flat[validate, ]$over_50k == &quot;1&quot;) 1 - sum(perc)/length(validate) #error of 0.361, not a great predictor. #creating and testing a random forest as predictor x = rep(NA, n) for (i in 1:n) { rf = randomForest(over_50k ~ ., mtry = 7, ntree = 100, data = flat, subset = train) rf2 = predict(rf, flat[validate,]) x[i] = 1 - sum(rf2 == flat[validate, ]$over_50k)/length(rf2) } hist(x) #mean error of 0.176, much better #creating and testing a naive bayes model to predict over_50k nb = naiveBayes(x= flat[, c(-14)], y = flat$over_50k, subset = train) p = predict(nb, flat[, c(-14)]) t = table(pred=p,true=flat$over_50k) 1 - ((t[1, 1] + t[2, 2])/sum(t)) #error of .172, basically same as the random forest. "]
]
