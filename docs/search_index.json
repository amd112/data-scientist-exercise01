[
["index.html", "Predicting Income From 1996 US Census Data", " Predicting Income From 1996 US Census Data Using an available US Census microdata sample from 1996, we attempt to predict if individuals have an income of above or below $50,000. Before broaching the ultimate modeling goal, it is important to understand the quirks of the data, and any data cleansing issues that must be addressed. Overall, the data doesn’t follow some patterns we expect. By comparing the 1996 microdata sample to the aggregations of surrounding decennial census’, we see this is likely not a random sample. There is significant underrepresentation of the black and ‘other’ race groups, as well as a clear overrepresentation of the male population (which composes 66.85% of the sample). Exploring other common trends in earnings, we see expected relationships. Among participants with more years of education, there is a higher percentage of respondents with incomes over 50k. Similarly, there is some relationship between hours worked and percentage of respondents with incomes over 50k. Using these (and other) relationships in the data, we model the probability of an individual record having an income of over 50k. The final model used is a Random Forest, a group of decision trees returning the classification that the most trees agree on. Each decision tree recursively splits the data by independent variables, choosing each split as the one which best seperates the dependent categories. The ultimate model has an error rate of 17.3% "],
["modeling.html", "Modeling", " Modeling In order to predict if income is above or below 50,000 we build predictive models. The initial models built include a logistic regression, a random forest, and a naive bayes classifier. Using these models on a training set and testing the results on a testing set, we find which models are initially competititve. #splitting the data in to train, validate, and testing groups s = 1:nrow(flat) train = sample(s, 12000) validate = sample(s[!s %in% train], 4000) test = s[(!s %in% train) &amp; (!s %in% validate)] n = 10 #creating and testing logisitic regression l = glm(over_50k ~ ., data = flat[train, ], family = binomial(link = &quot;logit&quot;)) p = predict(l, flat[validate, ], type = &quot;response&quot;) perc = (p&gt;0.05) == (flat[validate, ]$over_50k == &quot;1&quot;) #error of 0.361, not a great predictor. #creating and testing a random forest as predictor x = rep(NA, n) for (i in 1:n) { rf = randomForest(over_50k ~ ., mtry = 12, ntree = 30, data = flat, subset = train) rf2 = predict(rf, flat[validate,]) x[i] = 1 - sum(rf2 == flat[validate, ]$over_50k)/length(rf2) } #mean error of 0.176, much better #creating and testing a naive bayes model to predict over_50k nb = naiveBayes(x= flat[, c(-14)], y = flat$over_50k, subset = train) p = predict(nb, flat[validate, c(-14)]) t = table(pred=p,true=flat[validate,]$over_50k) #error of 0.176, same as the random forest. The logisitic regression model has an error rate of 0.361. The Random Forest has a mean error rate of 0.176325. The Naive Bayes has an error rate of 0.176. Given the much lower error rates of the Random Forest and Naive Bayes classifier, tuning the parameters of the forest is likely to give an improved error rate. After some tuning of parameters, the error rate of the model is 0.173 "],
["code.html", "Code", " Code #importing relevant libraries library(randomForest) library(e1071) library(ggplot2) library(dplyr) library(gridExtra) #setting seed to ensure that randomized calculations are constant set.seed(50) Code to flatten the database: library(DBI) library(RSQLite) con = dbConnect(SQLite(), dbname = &quot;exercise01.sqlite&quot;) dbExecute(con, &quot;CREATE TABLE flat (id INT PRIMARY KEY, age INT, workclass NCHAR(50), education NCHAR(50), education_id INT, marital_status NCHAR(50), occupation NCHAR(50), relationship NCHAR(50), race NCHAR(30), sex NCHAR(10), capital_gain INT, capital_loss INT, hours_week INT, country NCHAR(60), over_50k INT);&quot;) dbExecute(con, &quot;INSERT INTO flat SELECT records.id, records.age, w.name, e.name, records.education_num, m.name, o.name, re.name, r.name, s.name, capital_gain, capital_loss, hours_week, c.name, over_50k FROM countries AS c, education_levels AS e, marital_statuses AS m, occupations AS o, races AS r, records, relationships AS re, sexes AS s, workclasses AS w WHERE records.workclass_id = w.id AND records.education_level_id = e.id AND records.marital_status_id = m.id AND records.occupation_id = o.id AND records.relationship_id = re.id AND records.race_id = r.id AND records.sex_id = s.id AND records.country_id = c.id;&quot;) flat = dbGetQuery(con, &quot;SELECT * FROM flat;&quot;) write.csv(flat, file = &quot;flat.csv&quot;, row.names = FALSE) Code to bring in the flattened data: #importing file, removing column of ids flat = read.csv(&quot;flat.csv&quot;)[, -1] flat$over_50k = as.factor(flat$over_50k) Code to generate census image: #chunk is pushed right by the html wrapper #getting the summary of race distribution from the full data temp = as.data.frame(summary(flat$race)/nrow(flat)) names(temp) = c(&quot;percentages&quot;) temp$race = rownames(temp) rownames(temp) = c() temp$year = 1996 #appending the summaries of race distributions for 1990 and 2000 ninety = data.frame(percentages = c(0.008, 0.029, 0.121, 0.039, 0.803), race = temp$race, year = 1990) thou = data.frame(percentages = c(0.009, 0.037, 0.123, 0.08, 0.751), race = temp$race, year = 2000) temp = rbind(temp, ninety, thou) temp$year = as.factor(temp$year) #source 1990, page 3: https://www2.census.gov/library/publications/decennial/1990/cp-1/cp-1-1.pdf #source 2000, page 2: https://www.census.gov/prod/2002pubs/c2kprof00-us.pdf #plotting the summaries in comparison ggplot(temp) + geom_bar(aes(x = year, y = percentages, fill = race), position = &quot;stack&quot;, stat = &quot;identity&quot;) + ylim(0, 1) + xlab(&quot;Census Year&quot;) + ylab(&quot;Percent&quot;) Code to generate income plot: temp = flat %&gt;% select(over_50k, education_id) %&gt;% group_by(education_id) %&gt;% summarise(over_50k = sum(over_50k == &quot;1&quot;)/(sum(over_50k == &quot;1&quot;) + sum(over_50k == &quot;0&quot;))) plot1 = ggplot(data = temp) + geom_line(aes(x = education_id, y = over_50k)) + xlab(&quot;Years of Education&quot;) + ylab(&quot;% Earning Over 50k&quot;) + ylim(0, 0.8) temp2 = flat %&gt;% select(over_50k, hours_week) %&gt;% group_by(hours_week) %&gt;% summarise(over_50k = sum(over_50k == &quot;1&quot;)/(sum(over_50k == &quot;1&quot;) + sum(over_50k == &quot;0&quot;))) plot2 = ggplot(data = temp2) + geom_line(aes(x = hours_week, y = over_50k)) + xlab(&quot;Hours worked/Week&quot;) + ylab(&quot;% Earning Over 50k&quot;) + ylim(0, 0.8) + xlim(0, 70) grid.arrange(plot1, plot2, ncol=2) Code for final model, after tuning: rf = randomForest(over_50k ~ ., mtry = 3, ntree = 500, data = flat, subset = train) rf2 = predict(rf, flat[validate,]) Sample of flat file: ## age workclass education education_id marital_status ## 1 39 State-gov Bachelors 13 Never-married ## 2 50 Self-emp-not-inc Bachelors 13 Married-civ-spouse ## 3 38 Private HS-grad 9 Divorced ## 4 53 Private 11th 7 Married-civ-spouse ## 5 28 Private Bachelors 13 Married-civ-spouse ## 6 37 Private Masters 14 Married-civ-spouse ## occupation relationship race sex capital_gain capital_loss ## 1 Adm-clerical Not-in-family White Male 2174 0 ## 2 Exec-managerial Husband White Male 0 0 ## 3 Handlers-cleaners Not-in-family White Male 0 0 ## 4 Handlers-cleaners Husband Black Male 0 0 ## 5 Prof-specialty Wife Black Female 0 0 ## 6 Exec-managerial Wife White Female 0 0 ## hours_week country over_50k ## 1 40 United-States 0 ## 2 13 United-States 0 ## 3 40 United-States 0 ## 4 40 United-States 0 ## 5 40 Cuba 0 ## 6 40 United-States 0 "]
]
